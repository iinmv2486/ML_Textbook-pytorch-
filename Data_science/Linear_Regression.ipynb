{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = 2x_1 + 3x_2 -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_train = torch.FloatTensor([[1, 2],\n",
    "                             [3, 2],\n",
    "                             [3, 7],\n",
    "                             [1, 1],\n",
    "                             [1, 0]])\n",
    "\n",
    "y_train = torch.FloatTensor([[4],\n",
    "                             [8],\n",
    "                             [23],\n",
    "                             [1],\n",
    "                             [-2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 초기화\n",
    "W: 가중치\n",
    "\n",
    "b: 편향\n",
    "\n",
    "lr(learning Rate): 학습률\n",
    "\n",
    "질문:\n",
    "\n",
    "torch.rand()가 뭐 하는 함수지? 그리고 왜 가중치는 2, 1이고 편향은 1, 1이지? 벡터와 스칼라인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: \n",
      " tensor([[0.8554],\n",
      "        [0.0198]])\n",
      "b: \n",
      " tensor([[0.8660]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.rand(2, 1)\n",
    "b = torch.rand(1, 1)\n",
    "lr = 0.01\n",
    "\n",
    "print(\"W: \\n\", W)\n",
    "print(\"b: \\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 반복 횟수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, cost: 83.479477, W: tensor([1.1340, 0.6150]), b: tensor([[0.9529]])\n",
      "epoch: 100, cost: 1.822360, W: tensor([0.5931, 3.1222]), b: tensor([[-1.1318]])\n",
      "epoch: 200, cost: 0.870119, W: tensor([0.8452, 3.1940]), b: tensor([[-2.0449]])\n",
      "epoch: 300, cost: 0.446318, W: tensor([1.1556, 3.1511]), b: tensor([[-2.6099]])\n",
      "epoch: 400, cost: 0.229360, W: tensor([1.3928, 3.1097]), b: tensor([[-3.0047]])\n",
      "epoch: 500, cost: 0.117872, W: tensor([1.5645, 3.0788]), b: tensor([[-3.2867]])\n",
      "epoch: 600, cost: 0.060577, W: tensor([1.6878, 3.0565]), b: tensor([[-3.4886]])\n",
      "epoch: 700, cost: 0.031132, W: tensor([1.7762, 3.0405]), b: tensor([[-3.6334]])\n",
      "epoch: 800, cost: 0.015999, W: tensor([1.8395, 3.0290]), b: tensor([[-3.7372]])\n",
      "epoch: 900, cost: 0.008222, W: tensor([1.8850, 3.0208]), b: tensor([[-3.8116]])\n",
      "epoch: 1000, cost: 0.004226, W: tensor([1.9175, 3.0149]), b: tensor([[-3.8649]])\n",
      "epoch: 1100, cost: 0.002172, W: tensor([1.9409, 3.0107]), b: tensor([[-3.9032]])\n",
      "epoch: 1200, cost: 0.001116, W: tensor([1.9576, 3.0077]), b: tensor([[-3.9306]])\n",
      "epoch: 1300, cost: 0.000574, W: tensor([1.9696, 3.0055]), b: tensor([[-3.9502]])\n",
      "epoch: 1400, cost: 0.000295, W: tensor([1.9782, 3.0039]), b: tensor([[-3.9643]])\n",
      "epoch: 1500, cost: 0.000151, W: tensor([1.9844, 3.0028]), b: tensor([[-3.9744]])\n",
      "epoch: 1600, cost: 0.000078, W: tensor([1.9888, 3.0020]), b: tensor([[-3.9817]])\n",
      "epoch: 1700, cost: 0.000040, W: tensor([1.9920, 3.0015]), b: tensor([[-3.9869]])\n",
      "epoch: 1800, cost: 0.000021, W: tensor([1.9942, 3.0010]), b: tensor([[-3.9906]])\n",
      "epoch: 1900, cost: 0.000011, W: tensor([1.9959, 3.0007]), b: tensor([[-3.9932]])\n",
      "epoch: 2000, cost: 0.000005, W: tensor([1.9970, 3.0005]), b: tensor([[-3.9952]])\n",
      "epoch: 2100, cost: 0.000003, W: tensor([1.9979, 3.0004]), b: tensor([[-3.9965]])\n",
      "epoch: 2200, cost: 0.000001, W: tensor([1.9985, 3.0003]), b: tensor([[-3.9975]])\n",
      "epoch: 2300, cost: 0.000001, W: tensor([1.9989, 3.0002]), b: tensor([[-3.9982]])\n",
      "epoch: 2400, cost: 0.000000, W: tensor([1.9992, 3.0001]), b: tensor([[-3.9987]])\n",
      "epoch: 2500, cost: 0.000000, W: tensor([1.9994, 3.0001]), b: tensor([[-3.9991]])\n",
      "epoch: 2600, cost: 0.000000, W: tensor([1.9996, 3.0001]), b: tensor([[-3.9993]])\n",
      "epoch: 2700, cost: 0.000000, W: tensor([1.9997, 3.0001]), b: tensor([[-3.9995]])\n",
      "epoch: 2800, cost: 0.000000, W: tensor([1.9998, 3.0000]), b: tensor([[-3.9997]])\n",
      "epoch: 2900, cost: 0.000000, W: tensor([1.9999, 3.0000]), b: tensor([[-3.9998]])\n",
      "epoch: 3000, cost: 0.000000, W: tensor([1.9999, 3.0000]), b: tensor([[-3.9998]])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3001):\n",
    "    W.requires_grad_(True)\n",
    "    b.requires_grad_(True)\n",
    "\n",
    "    hypothesis = torch.mm(x_train, W) + b\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "\n",
    "    cost.backward()\n",
    "    with torch.no_grad() as grd:\n",
    "        W = W - lr * W.grad\n",
    "        b = b - lr * b.grad\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print( 'epoch: {}, cost: {:.6f}, W: {}, b: {}'.format(epoch, cost.item(), W.squeeze(), b))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.99983596801758\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.FloatTensor([[5, 10]])\n",
    "test_result = torch.mm(x_test, W) + b\n",
    "print(test_result.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 3.]] [-4.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = [[1,2], [3,2], [3,7], [1,1], [1,0]]\n",
    "y = [[4], [8], [23], [1], [-2]]\n",
    "\n",
    "model = LinearRegression() # 모델 생성\n",
    "model.fit(x, y)\n",
    "\n",
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36.]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([[5, 10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost: 462.169452\n",
      "Epoch 100, Cost: 0.314627\n",
      "Epoch 200, Cost: 0.257291\n",
      "Epoch 300, Cost: 0.210403\n",
      "Epoch 400, Cost: 0.172060\n",
      "Epoch 500, Cost: 0.140705\n",
      "Epoch 600, Cost: 0.115063\n",
      "Epoch 700, Cost: 0.094095\n",
      "Epoch 800, Cost: 0.076947\n",
      "Epoch 900, Cost: 0.062925\n",
      "Final Weights (W):\n",
      "[[1.82507301]\n",
      " [2.06552412]\n",
      " [2.20040963]]\n",
      "Final Bias (b): [1.07803247]\n",
      "Prediction for X_test: [[32.28373952]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_regression(X, y, lr=0.01, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    n차원의 입력 행렬 X와 출력 벡터 y에 대해 선형 회귀를 수행하는 함수.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): 입력 데이터 (샘플 수 x 특성 수)\n",
    "    y (numpy array): 출력 데이터 (샘플 수 x 1)\n",
    "    lr (float): 학습률 (learning rate), default = 0.01\n",
    "    num_epochs (int): 학습 반복 횟수 (epoch), default = 1000\n",
    "    \n",
    "    Returns:\n",
    "    W (numpy array): 학습된 가중치 (특성 수 x 1)\n",
    "    b (float): 학습된 편향 (bias)\n",
    "    \"\"\"\n",
    "    # 데이터 차원\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # 가중치(W)와 편향(b) 초기화\n",
    "    W = np.random.randn(n_features, 1)  # 특성 수에 맞는 가중치 초기화\n",
    "    b = np.random.randn(1)              # 편향 초기화\n",
    "    \n",
    "    # 경사 하강법을 통한 학습 과정\n",
    "    for epoch in range(num_epochs):\n",
    "        # 예측값 계산 (XW + b)\n",
    "        y_pred = np.dot(X, W) + b\n",
    "        \n",
    "        # 손실 함수 (MSE) 계산\n",
    "        cost = np.mean((y_pred - y) ** 2)\n",
    "        \n",
    "        # 가중치(W)와 편향(b)에 대한 그래디언트 계산\n",
    "        W_grad = (2 / n_samples) * np.dot(X.T, (y_pred - y))  # W에 대한 gradient\n",
    "        b_grad = (2 / n_samples) * np.sum(y_pred - y)         # b에 대한 gradient\n",
    "        \n",
    "        # 가중치와 편향 업데이트\n",
    "        W -= lr * W_grad\n",
    "        b -= lr * b_grad\n",
    "        \n",
    "        # 100 에포크마다 비용 출력\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Cost: {cost:.6f}')\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "# 사용 예시\n",
    "\n",
    "# 입력 데이터 (5개의 샘플, 3개의 특성)\n",
    "X = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [2, 3, 4],\n",
    "              [6, 7, 8]])\n",
    "\n",
    "# 출력 데이터\n",
    "y = np.array([[14],\n",
    "              [32],\n",
    "              [50],\n",
    "              [20],\n",
    "              [44]])\n",
    "\n",
    "# 선형 회귀 수행\n",
    "W, b = linear_regression(X, y, lr=0.001, num_epochs=1000)\n",
    "\n",
    "# 결과 출력\n",
    "print(f'Final Weights (W):\\n{W}')\n",
    "print(f'Final Bias (b): {b}')\n",
    "\n",
    "# 새로운 데이터 예측\n",
    "X_test = np.array([[3, 5, 7]])  # 새로운 입력 데이터\n",
    "y_test_pred = np.dot(X_test, W) + b\n",
    "print(f'Prediction for X_test: {y_test_pred}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
