{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = 2x_1 + 3x_2 -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_train = torch.FloatTensor([[1, 2],\n",
    "                             [3, 2],\n",
    "                             [3, 7],\n",
    "                             [1, 1],\n",
    "                             [1, 0]])\n",
    "\n",
    "y_train = torch.FloatTensor([[4],\n",
    "                             [8],\n",
    "                             [23],\n",
    "                             [1],\n",
    "                             [-2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 초기화\n",
    "W: 가중치\n",
    "\n",
    "b: 편향\n",
    "\n",
    "lr(learning Rate): 학습률\n",
    "\n",
    "질문:\n",
    "\n",
    "torch.rand()가 뭐 하는 함수지? 그리고 왜 가중치는 2, 1이고 편향은 1, 1이지? 벡터와 스칼라인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: \n",
      " tensor([[0.8554],\n",
      "        [0.0198]])\n",
      "b: \n",
      " tensor([[0.8660]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.rand(2, 1)\n",
    "b = torch.rand(1, 1)\n",
    "lr = 0.01\n",
    "\n",
    "print(\"W: \\n\", W)\n",
    "print(\"b: \\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 반복 횟수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, cost: 83.479477, W: tensor([1.1340, 0.6150]), b: tensor([[0.9529]])\n",
      "epoch: 100, cost: 1.822360, W: tensor([0.5931, 3.1222]), b: tensor([[-1.1318]])\n",
      "epoch: 200, cost: 0.870119, W: tensor([0.8452, 3.1940]), b: tensor([[-2.0449]])\n",
      "epoch: 300, cost: 0.446318, W: tensor([1.1556, 3.1511]), b: tensor([[-2.6099]])\n",
      "epoch: 400, cost: 0.229360, W: tensor([1.3928, 3.1097]), b: tensor([[-3.0047]])\n",
      "epoch: 500, cost: 0.117872, W: tensor([1.5645, 3.0788]), b: tensor([[-3.2867]])\n",
      "epoch: 600, cost: 0.060577, W: tensor([1.6878, 3.0565]), b: tensor([[-3.4886]])\n",
      "epoch: 700, cost: 0.031132, W: tensor([1.7762, 3.0405]), b: tensor([[-3.6334]])\n",
      "epoch: 800, cost: 0.015999, W: tensor([1.8395, 3.0290]), b: tensor([[-3.7372]])\n",
      "epoch: 900, cost: 0.008222, W: tensor([1.8850, 3.0208]), b: tensor([[-3.8116]])\n",
      "epoch: 1000, cost: 0.004226, W: tensor([1.9175, 3.0149]), b: tensor([[-3.8649]])\n",
      "epoch: 1100, cost: 0.002172, W: tensor([1.9409, 3.0107]), b: tensor([[-3.9032]])\n",
      "epoch: 1200, cost: 0.001116, W: tensor([1.9576, 3.0077]), b: tensor([[-3.9306]])\n",
      "epoch: 1300, cost: 0.000574, W: tensor([1.9696, 3.0055]), b: tensor([[-3.9502]])\n",
      "epoch: 1400, cost: 0.000295, W: tensor([1.9782, 3.0039]), b: tensor([[-3.9643]])\n",
      "epoch: 1500, cost: 0.000151, W: tensor([1.9844, 3.0028]), b: tensor([[-3.9744]])\n",
      "epoch: 1600, cost: 0.000078, W: tensor([1.9888, 3.0020]), b: tensor([[-3.9817]])\n",
      "epoch: 1700, cost: 0.000040, W: tensor([1.9920, 3.0015]), b: tensor([[-3.9869]])\n",
      "epoch: 1800, cost: 0.000021, W: tensor([1.9942, 3.0010]), b: tensor([[-3.9906]])\n",
      "epoch: 1900, cost: 0.000011, W: tensor([1.9959, 3.0007]), b: tensor([[-3.9932]])\n",
      "epoch: 2000, cost: 0.000005, W: tensor([1.9970, 3.0005]), b: tensor([[-3.9952]])\n",
      "epoch: 2100, cost: 0.000003, W: tensor([1.9979, 3.0004]), b: tensor([[-3.9965]])\n",
      "epoch: 2200, cost: 0.000001, W: tensor([1.9985, 3.0003]), b: tensor([[-3.9975]])\n",
      "epoch: 2300, cost: 0.000001, W: tensor([1.9989, 3.0002]), b: tensor([[-3.9982]])\n",
      "epoch: 2400, cost: 0.000000, W: tensor([1.9992, 3.0001]), b: tensor([[-3.9987]])\n",
      "epoch: 2500, cost: 0.000000, W: tensor([1.9994, 3.0001]), b: tensor([[-3.9991]])\n",
      "epoch: 2600, cost: 0.000000, W: tensor([1.9996, 3.0001]), b: tensor([[-3.9993]])\n",
      "epoch: 2700, cost: 0.000000, W: tensor([1.9997, 3.0001]), b: tensor([[-3.9995]])\n",
      "epoch: 2800, cost: 0.000000, W: tensor([1.9998, 3.0000]), b: tensor([[-3.9997]])\n",
      "epoch: 2900, cost: 0.000000, W: tensor([1.9999, 3.0000]), b: tensor([[-3.9998]])\n",
      "epoch: 3000, cost: 0.000000, W: tensor([1.9999, 3.0000]), b: tensor([[-3.9998]])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3001):\n",
    "    W.requires_grad_(True)\n",
    "    b.requires_grad_(True)\n",
    "\n",
    "    hypothesis = torch.mm(x_train, W) + b\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "\n",
    "    cost.backward()\n",
    "    with torch.no_grad() as grd:\n",
    "        W = W - lr * W.grad\n",
    "        b = b - lr * b.grad\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print( 'epoch: {}, cost: {:.6f}, W: {}, b: {}'.format(epoch, cost.item(), W.squeeze(), b))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.99983596801758\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.FloatTensor([[5, 10]])\n",
    "test_result = torch.mm(x_test, W) + b\n",
    "print(test_result.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
